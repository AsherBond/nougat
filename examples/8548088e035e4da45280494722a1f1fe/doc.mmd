# Neural Attentive Circuits

Nasim Rahaman\({}^{*,1,2}\) Martin Weiss\({}^{*,1,6}\) Francesco Locatello\({}^{3}\) Chris Pal \({}^{1,6,7}\)

Yoshua Bengio\({}^{1,5,7}\) Bernhard Scholkopf\({}^{2}\) Li Erran Li\({}^{\dagger,3}\) Nicolas Ballas \({}^{\dagger,4}\)

\({}^{*,\dagger}\) Equal contribution, random order.

\({}^{1}\)Mila, Quebec AI Institute

\({}^{2}\) Max Planck Institute for Intelligent Systems, Tubingen

\({}^{3}\) AWS AI \({}^{4}\) Meta AI

\({}^{5}\) Universite de Montreal \({}^{6}\) Polytechnique Montreal

\({}^{7}\) Canada CIFAR AI Chair

###### Abstract

Recent work has seen the development of general purpose neural architectures that can be trained to perform tasks across diverse data modalities. General purpose models typically make few assumptions about the underlying data-structure and are known to perform well in the large-data regime. At the same time, there has been growing interest in modular neural architectures that represent the data using sparsely interacting modules. These models can be more robust out-of-distribution, computationally efficient, and capable of sample-efficient adaptation to new data. However, they tend to make domain-specific assumptions about the data, and present challenges in how module behavior (i.e., parameterization) and connectivity (i.e., their layout) can be jointly learned. In this work, we introduce a general purpose, yet modular neural architecture called Neural Attentive Circuits (NACs) that jointly learns the parameterization and a sparse connectivity of neural modules without using domain knowledge. NACs are best understood as the combination of two systems that are jointly trained end-to-end: one that determines the module configuration and the other that executes it on an input. We demonstrate qualitatively that NACs learn diverse and meaningful module configurations on the Natural Language and Visual Reasoning for Real (NLVR2) dataset without additional supervision. Quantitatively, we show that by incorporating modularity in this way, NACs improve upon a strong non-modular baseline in terms of low-shot adaptation on CIFAR and Caltech-UCSD Birds dataset (CUB) by about 10 percent, and OOD robustness on Tiny ImageNet-R by about 2.5 percent. Further, we find that NACs can achieve an 8x speedup at inference time while losing less than 3 percent performance. Finally, we find NACs to yield competitive results on diverse data modalities spanning point-cloud classification, symbolic processing and text-classification from ASCII bytes, thereby confirming its general purpose nature.

## 1 Introduction

General purpose neural models like Perceivers [29] do not make significant assumptions about the underlying data-structure of the input and tend to perform well in the large-data regime. This enables the application of the same model on a variety of data modalities, including images, text, audio, point-clouds, and arbitrary combinations thereof [29, 28]. This is appealing from an ease-of-use perspective, since the amount of domain-specific components is minimized, and the resulting models can function well _out-of-the-box_ in larger machine learning pipelines, e.g., AlphaStar [28].

At the same time, natural data generating processes can often be well-represented by a system of sparsely interacting independent mechanisms [41, 46], and the Sparse Mechanism Shift hypothesisstipulates that real-world shifts are often sparse when decomposed, i.e., that most mechanisms may remain invariant [47]. Modular architectures seek to leverage this structure by enabling the learning of systems of sparsely interacting neural modules [3, 20, 44, 45]. Such systems tend to excel in low-data regimes, systematic generalization, fast (sample-efficient) adaptation to new data and can maintain a larger degree of robustness out-of-distribution [52, 3, 4, 21]. However, many of these models make domain-specific assumptions about the data distribution and modalities [4] - e.g., Mao et al. [37] relying on visual scene graphs, or Andreas et al. [3] using hand-specified modules and behavioral cloning. As a result, such models are often less flexible and more difficult to deploy.

In this work, we propose Neural Attentive Circuits (NACs), an architecture that incorporates modular inductive biases while maintaining the versatility of general purpose models. NACs implement a system of sparsely and attentively interacting neural modules that pass messages to each other along _connectivity graphs_ that are learned or dynamically inferred in the forward pass and subject to priors derived from network science [25, 6, 18]. NACs demonstrate strong low-shot adaptation performance and better out-of-distribution robustness when compared to other general purpose models such as Perceiver IOs [28]. Besides scaling linearly with input size (like Perceivers), we show that by pruning modules, the computational complexity of NACs can be reduced at inference time while preserving performance. In addition, we propose a conditional variant where the graph structure and module parameterization is conditioned on the input. This enables conditional computation [10, 9], where the functions computed by neural modules and the sparse connectivity between modules is determined only at inference time. A qualitative analysis on Natural Language and Visual Reasoning for Real (NLVR2) shows that conditional NACs can learn connectivity graphs that are meaningfully diverse.

Figure 1 shows the schematics of the proposed architecture, with its two core components: the **circuit generator** and the **circuit executor**. The circuit generator produces the configuration over modules, which we call a circuit design, defining (a) the connectivity pattern between the neural modules, and (b) instructions that condition the computation performed by each module. The circuit design may either be conditioned on (part of) the sample (in the case of conditional NACs), or it may simply be learned over the course of training via gradient descent (in the case of unconditional NACs). The circuit executor consumes the circuit design and an input sample to perform inference.

**Our contributions are as follows. (a)** We propose Neural Attentive Circuits (NACs), a general purpose neural architecture that jointly learns the module parameterization and connectivity end-to-end and gracefully supports more than one thousand sparsely interacting modules. (b) We demonstrate the general-purpose nature of NACs by training it on a selection of diverse data domains covering natural images, 3D point-clouds, symbolic processing, text understanding from raw ASCII bytes, and natural-language visual reasoning. (c) We quantitatively evaluate the out-of-distribution robustness and few-shot adaptation performance of NACs. We find that in the low-shot regime, NACs can outperform Perceiver IOs by approximately 10% on 8-Way CIFAR and CUB-2011. Further, the proposed model yields roughly 2.5% improvement on TinyImageNet-R, an out-of-distribution test-set for TinyImageNet based on ImageNet-R [24]. (d) We explore the adaptive-computation aspect of NACs, wherein the computational complexity of a trained model can be reduced at inference time by roughly 8 times, at the cost of less than 3% loss in accuracy on Tiny-ImageNet. (e) We qualitatively demonstrate that it is possible to condition the circuit design (i.e., configuration over modules) on the input. On NLVR2, we use the text modality of the sample to condition the circuit design, which is

Figure 1: **Circuit Generator and Executor.** We show NAC applied to a natural language and visual reasoning task where text conditions the circuit generator and the program executor consumes images.

then applied to the image modality of the same sample to produce a result. We find that connectivity graphs generated by sentences that involve similar reasoning skills have similar structures.

## 2 Neural Attentive Circuits

This section describes Neural Attentive Circuits (NACs), a neural architecture that can consume arbitrary (tokenized) set-valued inputs including images and text. NACs are a system of neural modules that learn to interact with each other (see Figure 2). Module connectivity is learned end-to-end and is subject to regularization that encourages certain patterns (e.g., sparsity, scale-freeness and formation of cliques). In the following, we first describe two key building blocks of the proposed architecture. We then introduce the circuit executor that infers an output given some input and a circuit design. We finally introduce the circuit generator which produces said circuit design.

**Circuit Design.** Consider a NAC (Figure 2) with \(U\) modules, of which \(U_{p}\) are _processor modules_ (responsible for the bulk of the computation) and \(U_{o}\) are _read-out modules_ (responsible for extracting model outputs). By circuit design, we refer to the set of \(U\) descriptors that condition the module connectivity and computation carried out at each module. Each descriptor comprises three vectors (\(\mathbf{s}_{u}\), \(\mathbf{c}_{u}\), \(\mathbf{\theta}_{u}^{(0)}\)): a signature vector \(\mathbf{s}_{u}\in\mathbb{R}^{d_{orig}}\), a code vector \(\mathbf{c}_{u}\in\mathbb{R}^{d_{code}}\) and a module initial state \(\mathbf{\theta}_{u}^{(0)}\in\mathbb{R}^{d_{model}}\). The set of signature vectors \(\{\mathbf{s}_{u}\}_{u}\) determines the connectivity between modules while the code vector \(\mathbf{c}_{u}\) conditions the computation performed by module at index \(u\). Further, the vector \(\mathbf{\theta}_{u}^{(l)}\in\mathbb{R}^{d_{model}}\) tracks the state of the module \(u\) before the \(l\)-th layer, and by \(\Theta^{(l)}\) we denote the set of all such states (over modules). Each module updates its state as the computation progresses in depth (see below). Finally, processor module signatures and codes are shared across depth.

**ModFC and ModFFN.** The Modulated Fully-Connected (ModFC) layer is the component that enables a module to condition its computation on a code. It replaces the fully-connected layer and supports a multiplicative conditioning of the input vector \(\mathbf{x}\) by the _code vector_\(\mathbf{c}\). It can be expressed

\[\mathbf{y}=\text{ModFC}(\mathbf{x};\mathbf{c})=\mathbf{W}(\mathbf{x}\odot(1+\alpha\,\text{LayerNorm}( \mathbf{W}_{c}\mathbf{c})))+\mathbf{b}, \tag{1}\]

where \(\odot\) denotes element-wise multiplication, \(\mathbf{W}\in\mathbb{R}^{d_{out}}\times\mathbb{R}^{d_{in}}\) is a weight matrix, \(\mathbf{b}\in\mathbb{R}^{d_{out}}\) is a bias vector, \(\mathbf{c}\in\mathbb{R}^{d_{code}}\) is the conditioning code vector, \(\mathbf{W}_{c}\in\mathbb{R}^{d_{in}}\times\mathbb{R}^{d_{code}}\) is the conditioning weight matrix, and \(\alpha\) is a learnable scalar parameter that controls the amount of conditioning. Setting \(\alpha=0\) removes all conditioning on \(\mathbf{c}\), and we generally initialize \(\alpha=0.1\). Multiple such ModFC layers (sharing the same code vector \(\mathbf{c}\)) and activation functions can be stacked to obtain a ModFFN

Figure 2: **Schematics of Neural Attentive Circuits.**_Top:_ a NAC with \(U_{p}=4\) processor modules and \(U_{o}=3\) read-out modules. _Bottom right:_ each module has a unique signature and code (colored circles and boxes). The signatures determine which modules interact and the code given to a module conditions the computation it performs (via ModFFNs and ModFCs). _Bottom left:_ visual description of ModFFNs and ModFC.

layer. Like in transformers, multiple copies of ModFCs and ModFFNs are executed in parallel. But unlike in transformers, each copy is conditioned by a unique learnable code vector and therefore performs a different computation. Consequently, though the computation performed in each module is different, the total number of parameters does not noticeably increase with the number of modules. See a clarifying example in Appendix B.

**SKMDPA.** Stochastic Kernel Modulated Dot-Product Attention (SKMDPA) is a sparse-attention mechanism that allows the modules to communicate. Given two modules at index \(i\) and \(j\), the distance between their signatures \(\mathbf{s}_{i}\) and \(\mathbf{s}_{j}\) determines the probability with which these modules can interact via dot-product attention - smaller distance implies higher likelihood that the modules are allowed to communicate. Like vanilla dot-product attention, SKMDPA operates on a set of query \(\mathbf{q}_{i}\), key \(\mathbf{k}_{j}\), and value \(\mathbf{v}_{j}\) vectors, where \(i\) indexes queries and \(j\) indexes keys and values. But, in addition, each query \(\mathbf{q}_{i}\) and key \(\mathbf{k}_{j}\) is equipped with a signature embedding vector, which may be learned as parameters or dynamically inferred. These signatures are used to sample a kernel matrix \(K\)[1] via

\[K_{ij}\sim\text{Concrete}(P_{ij},\tau)\text{ with }P_{ij}=\exp\left[\frac{-d(\mathbf{s}_ {i},\mathbf{s}_{j})}{\epsilon}\right], \tag{2}\]

where \(d\) is some distance metric, \(\epsilon\) is a bandwidth (a hyper-parameter), Concrete is the continuous relaxation of the Bernoulli distribution [36] with sampling temperature \(\tau\), and the sampling operation \(\sim\) is differentiable via the reparameterization trick. \(K_{ij}\) is likely to be close to \(1\) if the distance between \(s_{i}\) and \(s_{j}\) is small, and close to \(0\) if not. As \(\tau\to 0\), the kernel matrix \(K_{ij}\) becomes populated with either \(1\)s or \(0\)s. Further, if \(P_{ij}\) is ensured to be sufficiently small on average, \(K_{ij}\) is a sparse matrix. In practice, we set \(\tau\) to a small but non-zero value (e.g. \(0.5\)) to allow for exploration. Now, where \(A_{i\gets j}\) is the dot-product attention score \(\nicefrac{{\mathbf{q}_{i}\cdot\mathbf{k}_{j}}}{{\sqrt{d}}}\) between the query \(\mathbf{q}_{i}\) and key \(\mathbf{k}_{j}\), the net attention weight \(W_{i\gets j}\) and the output \(\mathbf{y}_{i}\) are

\[W_{i\gets j}=\text{softmax}_{j}\left[A_{i\gets j}+\log\hat{K}_{ij} \right]\text{ where }\hat{K}_{ij}=\nicefrac{{K_{ij}/(\delta+\sum_{j}K_{ij})}}{{ \left(\delta+\sum_{j}K_{ij}\right)}}\text{ and }\mathbf{y}_{i}=\sum_{j}W_{i\gets j}\mathbf{v}_{j}. \tag{3}\]

Here, \(W_{i\gets j}\) denotes the weight of the _message_\(\mathbf{v}_{j}\) passed by element \(j\) to element \(i\), and \(\delta\) is a small scalar for numerical stability. We note that if \(K_{ij}\approx 0\), we have \(W_{i\gets j}\approx 0\), implying if \(K_{ij}\) is sparse, so is \(W_{i\gets j}\). The proposed attention mechanism effectively allows the query \(\mathbf{q}_{i}\) to interact with the key \(\mathbf{k}_{j}\) with a probability that depends on the distance between their respective signatures. In this work, this distance is derived from the cosine similarity, i.e., \(d(\mathbf{s}_{i},\mathbf{s}_{j})=1-\text{CosineSimilarity}(\mathbf{s}_{i},\mathbf{s}_{j})\). We note the connection to Lin et al. [34], where the attentive interactions between queries and keys are dropped at random, albeit with a probability that is not differentiably learned.

### Circuit Executor

The circuit executor takes the circuit design and executes it on an input to make a prediction. It has four components: **(a)** a tokenizer to convert the input (e.g., images or text) to a set of vectors, **(b)** a _read-in_ layer which allows the processor modules to attentively read from the inputs, **(c)** a sequence of _propagator layers_ which iteratively update the processor modules states through a round of communication and computation, **(d)**_read-out_ layers that enable the read-out modules to attentively poll the final outputs of the processor modules.

**Tokenizer.** The tokenizer is any component that converts the input to the overall model to a set of representation vectors (with positional encodings, where appropriate), called the input set \(X\). It can be a learned neural network (e.g., the first few layers of a ConvNet), or a hard-coded patch extractor. _Role:_ The tokenizer standardizes the inputs to the model by converting the said inputs to a set of vectors.

**Read-In.** The read-in component is a read-in attention followed by a ModFFN. Read-in attention is a cross-attention that uses the initial processor module states \(\Theta_{p}^{(0)}\) to generate a query vector and uses the inputs \(X\) to generate keys and values. Unlike typical cross-attention [32, 29], we use ModFCs instead of FCs as query, key and value projectors. Where \(\mathbf{x}_{j}\) is an element of \(X\), we have:

\[\mathbf{q}_{u}=\text{ModFC}(\mathbf{\theta}_{u}^{(0)},\mathbf{c}_{u}) \mathbf{k}_{uj}=\text{ModFC}(\mathbf{x}_{j},\mathbf{c}_{u}) \mathbf{v}_{uj}=\text{ModFC}(\mathbf{x}_{j},\mathbf{c}_{u}) \tag{4}\] \[\mathbf{\hat{y}}_{\mathbf{u}}=\sum_{j}\frac{\mathbf{q}_{u}\cdot\mathbf{k}_{uj}}{ \sqrt{d}}\mathbf{v}_{uj} \mathbf{y}_{u}=\text{ModFC}(\mathbf{\hat{y}}_{\mathbf{u}},\mathbf{c}_{u}) \mathbf{\theta}_{u}^{(1)}=\text{ModFFN}(\mathbf{y}_{u},\mathbf{c}_{u}) \tag{5}\]Each query \(\mathbf{\theta}_{u}^{(0)}\) sees its own set of keys \(\mathbf{k}_{u,j}\) and values \(\mathbf{v}_{u,j}\), owing to the conditioning on the code \(\mathbf{c}_{u}\). The read-in attention is followed by \(U_{p}\) copies of a two layer ModFFN each conditioned on a code \(\mathbf{c}_{u}\). The read-in outputs a set of vectors \(\mathbf{\theta}_{u}^{(1)}\in\Theta_{p}^{(1)}\). _Role:_ The read-in attends to the inputs via a cross-attention mechanism that scales linearly with the size of the input.

**Propagator Layers.** After the read-in operation, propagator layers sequentially update the state of each processor module \(L\) times. Each propagator layer implements a round of module communication via SKMDPA, followed by the computation of \(U_{p}\) copies of a ModFFN. Both operations are conditioned on their processor module descriptor codes. The \(l\)-th propagator (where \(l=1,\ldots,L\)) ingests the set of states \(\Theta_{p}^{(l)}\) and outputs another set \(\Theta_{p}^{(l+1)}\) of the same size. _Role:_ A propagator layer updates each processor module state vector by applying learned stochastic attention and module computation.

**Read-Out.** The read-out component is a SKMDPA-based cross-attention mechanism (read-out attention) followed by a ModFFN, and it is responsible for extracting the output from the processor module states after the final propagator layer. Read-out modules have their own signatures and codes parameters which are different from the processor modules. The read-out attention uses the initial states \(\Theta_{o}^{(0)}\) of these modules as queries, and the final states of the processor modules \(\Theta_{p}^{(L+1)}\) to obtain keys and values. Finally, we note that in vanilla classification problems, we found it useful to use several read-out modules. Each read-out module produces a confidence score, which is used to weight their respective additive contribution to the final output (e.g., classification logits). _Role:_ The read-out is responsible for attending to the final processor module states to produce an overall output.

### Circuit Generator

Recall that the circuit generator produces a circuit design that specifies the connectivity between the modules (via signatures) and the computation performed by each module (via codes). There are two ways in which the circuit generator can be instantiated, as described below.

**Unconditional Circuit Generator.** In the first variant (unconditional), the circuit design is induced by signatures \(\mathbf{s}_{u}\) and codes \(\mathbf{c}_{u}\) that are freely learnable parameters (where \(u\) indexes the module descriptor). The initial state \(\mathbf{\theta}_{u}^{(0)}\) is obtained passing \(\mathbf{c}_{u}\) through a two layer MLP. We refer to the overall model as an unconditional NAC, or simply a NAC. _Role:_ The unconditional circuit generator implements a mechanism to learn the circuit design via free parameters with gradient descent.

**Regularization via Graph Priors.** In the unconditional setting, we observe a task-driven pressure that collapsed all signatures \(s_{u}\) to the same value (see Appendix A). This results in a graph that is fully connected, i.e., all modules interact with all other modules, thereby forgoing the benefits of sparsely interacting modules. To counteract this pressure, we impose a structural prior on the learned graph via a regularization term in the objective, which we call the Graph Structure Regularizer. The graph prior is expressed as a target link probability matrix and the extent to which the prior is satisfied is obtained comparing the closest permutation of \(P_{ij}\) (the generated link probability matrix) to the target values. This regularizer encourages the graph to have certain chosen properties, while still preserving the flexibility afforded by learning the graph structure. We experiment with several such properties, including scale-freeness [6] (leading to a few well connected nodes or _hubs_, and a heavy tail of less connected nodes), and a planted partition [15] one based on the stochastic block model [25] (encouraging modules to group themselves in _cliques,_ where communication within cliques is less constrained than that between cliques). In Section 4, we will find that some graph properties do indeed yield better out-of-distribution performance, whereas others perform better in-distribution. Figure 3 shows the class of graphs we experiment with in this work, and Appendix A contains the details. _Role:_ The Graph Structure Regularizer enforces sparsity by preventing the module connectivity graph from collapsing to an all-to-all connected graph, while inducing graph properties that may unlock additional performance depending on the task.

**Conditional Circuit Generator.** In the second variant (conditional), each sample is assigned a different circuit design. In this case, the circuit generator takes as input the sample \(X\) (or a part thereof), and outputs a set of signatures and codes \(\{\mathbf{s}_{u}(X),\mathbf{c}_{u}(X)\}_{u}\). This setting is particularly interesting for multi-modal data, where the circuit generator may ingest one modality and the circuit executor may consume another. In this work, we implement the conditional circuit generator as a simple cross attention mechanism, where learned parameters are used as queries (one per module) and elements of \(X\) as keys and values [32]. We refer to the resulting model as conditional NACs,while noting that the circuit generator could in principle be implemented by other means (e.g., auto-regressive models and GFLowNets [12]) in future work. _Role:_ The conditional circuit generator relies on the sample when producing a circuit design. This way, the connectivity between modules and the computation performed by each module can be dynamically changed for each sample.

**In summary,** we have introduced Neural Attentive Circuits (NACs) and its conditional variant, a general purpose neural architecture of attentively interacting modules. We discussed the two key components of NACs: **(a)** a circuit generator that _generates_ a circuit design determining which modules connect to which others (via signatures) and the function performed by each (via codes), and **(b)** a circuit executor that _executes_ the configuration on an input to infer an output.

## 3 Related work

**General Purpose Models.** A recent line of work develops _general-purpose models_ that make few assumptions about the data domain they are applied to [29, 28, 23]. Of these models, Perceiver IO [28] is most similar to NACs, in that **(a)** they both rely on a cross-attention to both read (tokenized) set-valued inputs into the model and extract set-valued output from the model, and **(b)** their computational complexity scales linearly with the input size. However, unlike Perceiver IO (PIO), **(a)** the connectivity between NAC modules is not all-to-all, but sparse in a learned way, and **(b)** the computation performed by NAC's equivalent of a PIO latent is conditioned at all layers via the ModFC component.

**Modular Inductive Biases.** Modular neural architectures are known to yield models that are capable of systematic generalization and fast adaptation [5, 2]. Of these architectures, Neural Interpreters (NIs) are most similar to NACs, in that they both use the framework of signatures and codes, where signatures determine the computational pathway and codes condition individual modules. However, there are four key differences: **(a)** unlike in NIs, the routing of information in NACs is stochastic, **(b)** the mechanism to condition on codes (ModFC) is different, and **(c)** the computational complexity and memory requirements of NIs scales quadratically with the size of the input, whereas in NACs, this scaling is linear, and **(d)** NIs are only known to work with up to 8 modules, whereas NACs can support more than a thousand modules on a single GPU (i.e., without model parallelism).

**Neuro-symbolic Models.** Neuro-symbolic models have shown strong systematic generalization on synthetic datasets such as CLEVR [30] and SQOOP [5]. However, models of this class such as the Neuro-Symbolic Concept Learner [37] and Neural Module Networks [3] are not general-purpose, using semantic parsers and domain-specific modules. In contrast, NACs are general-purpose and can jointly learn a sparse layout and parameterization for more than one thousand homogeneous modules.

## 4 Experiments

In this section, we empirically investigate Neural Attentive Circuits (NACs) in five problem settings with different modalities: image classification, point cloud classification, symbolic processing (ListOps [49]), text classification from raw bytes [49], and multi-modal reasoning over natural language and images [48]. We compare against baselines from the literature, as well as those that we train under controlled settings. The questions we ask are the following: **(a)** Does the inductive bias of sparsely interacting modules improve NACs out-of-distribution robustness and fast (few-shot) adaptation to new data? **(b)** How important is it that these modules be capable of modelling different functions (via ModFC layers), and that the connectivity between modules is learned (via SKMDPA)? **(c)** Do the modules strongly co-adapt to each other, or can the system still function with some modules

Figure 3: Samples from **Graph priors** explored here. Node colors represent degree (# of edges). **Left**: Planted-partition [15, 25]. **Center-left**: Scale-free [7, 6]. **Center-right**: Ring-of-Cliques [38]. **Right**: Erdos-Renyi [17].

pruned at inference time? **(d)** In the sample-conditional setting, are there interesting patterns in the generated circuit designs? **(e)** Are NACs general-purpose?

### Image Classification

In this section, we conduct a quantitative evaluation of NACs on image classification problems. We use Tiny-ImageNet as our primary test-bed. Tiny-ImageNet is a miniaturized version of ImageNet[16], with smaller dataset size and lower image resolution. It allows for compute efficient experimentation, but makes the overall classification task significantly more challenging, especially for larger models that can easily overfit [33]. To evaluate out-of-distribution (OOD) robustness in this regime, we use Tiny-ImageNet-R, which is a down-sampled subset of ImageNet-R(conditions) [24]. It contains roughly \(12000\) samples categorized in 64 classes (a subset of Tiny-ImageNet classes), spread across multiple visual domains such as art, cartoons, sculptures, origami, graffiti, embroidery, etc. For low-shot adaptation experiments, we also train a NAC on ImageNet, in order to compare with a pretrained baseline. Additional details can be found in Appendix C.

**Baselines.** For a certain choice of hyper-parameters, NACs can simulate Perceiver IOs (PIOs) [28], making the latter the natural baseline for NACs. For all Tiny-ImageNet runs for both NACs and PIOs, we use the same convolutional preprocessor that reduces the \(64\times 64\) input image to a \(8\times 8\) feature map. For ImageNet training, we use the same convolutional preprocessors as we do for Tiny-ImageNet. Please refer to Appendix C for additional details.

**Pre-training.** We pretrain all models on Tiny-ImageNet for 400 epochs, and perform model selection based on accuracy on the corresponding validation set. We evaluate the selected models on Tiny-ImageNet-R. For few-shot adaptation experiments, we use the official ImageNet pretrained weights for a 48-layer deep PIO [28], which yields 82.1% validation accuracy. To compare few-shot adaptation performance with this model, we train a 8-layer deep NAC with a scale-free prior graph on full ImageNet for 110 epochs, which yields 77% validation accuracy with 1024 modules. Additional details and hyperparameters can be found in Appendix C.

**Few-Shot Adaptation.** Bengio et al. [11] proposed that models that learn a good modularization of the underlying data generative process should adapt in a sample-efficient manner to unseen but related data-distributions. Motivated by this reasoning, we measure NACs few-shot adaptation performance and compare with a non-modular PIO baseline. To this end, we fine-tune ImageNet pre-trained NACs and PIOs on small numbers of samples - between 8 and 128 in total - from two different datasets: CUB-2011 (Birds) and CIFAR. The results shown in Figure 4 support the hypothesis that the modular inductive biases in NACs help with sample-efficient adaptation, and we show the few-shot experiments with varying numbers of classes in the Appendix with similar results.

**Ablations.** In this experiment (Table 1), we start with a Perceiver IO and cumulatively add design elements to arrive at the NAC model. We evaluate the IID and OOD performances and make two key observations. **(a)** We find that adding ModFC layers does not significantly affect IID generalization, but improves OOD generalization. Moreover, using a static Barabasi-Albert attention graph (via frozen signatures) reduces the IID performance while further improving OOD performance. Jointly

Figure 4: Few-Shot Adaptation Results on CUB and CIFAR. On each dataset, we fine-tune ImageNet pretrained NAC and PIO models to classify images into 8 classes and vary the number of examples (“shots”) per class from 1 to 16. In the low-shot transfer regime (e.g., 1-4 samples per class), we find that NAC compares favorably with the baseline PIO, suggesting that the modular inductive biases in the former can aid fast adaptation.

learning the connectivity graph and module codes yields the largest improvements. Learning the graph allows to recover the IID performance, and the OOD performance is further improved. **(b)** The choice of a graph prior exposes a trade-off between IID and OOD performance. The scale-free prior yields the best IID performance, whereas the ring-of-cliques prior yields the best OOD performance.

**Effect of Dropping Modules.** In this experiment, we seek to measure the extent to which the NACs modules depend on other modules to function. To this end, we drop modules in order of their connectivity (i.e., least connected modules are dropped first) and measure the performance of the model on Tiny-ImageNet along the way. We perform this experiment for models trained with different types of graph priors, and show the result in Figure 5. We make two observations: **(a)** it is possible to remove a large fraction (e.g., more than 80%) of the modules at little cost to accuracy, obtaining a \(\sim 8\times\) speedup in the process, and **(b)** different graph priors offer different amounts of robustness towards dropped modules. The former observation is particularly interesting from a computational efficiency perspective (Figure 4(b)), since dropping modules increases throughput and decreases memory requirements at inference-time without additional implementation overhead. The latter observation suggests a way in which the choice of a graph prior can affect the properties of the NACs model. We provide additional details in Appendix F, where Figure 15 compares the effect of sparsification on NACs (with different graph priors) and Perceiver IO.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{2}{c}{**IID**} & \multicolumn{2}{c}{**OOD**} \\  & (Tiny-ImageNet Validation) & \multicolumn{2}{c}{(Tiny-ImageNet-R)} \\ \hline  & Acc@1 & Acc@5 & Acc@1 & Acc@5 \\ \hline Perceiver IO & 58.89 & 80.09 & 17.57 & 37.16 \\ + ModFC & 58.91 & 79.43 & 18.01 & 37.05 \\ + Sparse graph (not learnable) & 58.31 & 80.06 & 18.50 & 37.92 \\ \hline NAC with Scale-Free Prior & **60.76** & 80.86 & 19.52 & 38.52 \\ NAC with Planted-Partition Prior & 60.71 & **81.34** & 19.42 & 39.84 \\ NAC with Ring-of-Cliques Prior & 60.54 & 81.18 & **20.03** & **40.44** \\ NAC with Erdos-Renyi Prior & 60.33 & 81.08 & 19.83 & 39.32 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Ablation and Graph Results.** In this experiment, we start with a Perceiver IO and cumulatively add design elements to arrive at the NAC model. At each stage, we train and observe the effect on generalization. We find that adding ModFC layers alone does not have a clear impact on either IID or OOD generalization. However, OOD is then clearly improved by adding a sparse kernel sampled from a frozen prior distribution. Below the midrule, we evaluate NACs performance with several different graph prior regularizers. We see that learning the graph via SKMDPA allows the model to improve both its IID and OOD performance.

Figure 5: **Adaptive Computation**. We train models to convergence and then sparsify them at inference time and evaluate their validation accuracy. In both plots, the x-axis shows the number of modules dropped at inference time, and the Y-axis shows the accuracy of the model on Tiny-ImageNet after sparsification at inference time. We observe that the Neural Attentive Circuit (NAC) is much more robust than Perceiver IO to sparsification at inference time, and can drop over 80% of its circuits before its performance drops below Perceiver IO. As a result, NACs can be adapted at inference time to a wide range of computational budgets.



### Point Cloud Classification

We evaluate the NAC architecture on a point cloud classification benchmark without additional data augmentation, geometric inductive biases or self-supervised pre-training [14]. Comparably, Perceiver IO obtained 77.4%, Hierarchical Perceivers achieved 75.9% without self-supervised pre-training, and when trained with self-supervised pre-training Hierarchical Perceivers obtained 80.6% (cf. Table 7 of [14]). We trained a NAC without specialized tokenization or pre-training to achieve a test accuracy of 83%, demonstrating that the sparse inductive prior was quite effective in this setting. The original Perceiver [29] relies on additional data augmentation beyond that used in [14] to obtain 85% test accuracy. We do not know how well the original Perceiver would perform without the additional augmentations, but expect it to be similar to Perceiver IO.

### Symbolic Processing on ListOps and Text Classification from Raw Bytes

In this set of experiments, we evaluate the ability of NACs to reason over raw, low-level inputs. We use two tasks to this end: a symbolic processing task (ListOps) and a text-classification task from raw ASCII bytes. The former (ListOps) is a challenging 10-way classification task on sequences of up to 2,000 characters [39]. This commonly used benchmark [49] tests the ability of models to reason hierarchically while handling long contexts. The latter is a binary sentiment classification task defined on sequences of up to 4000 bytes. In particular, we stress that the models ingest raw ASCII bytes, and **not** tokenized text as common in most NLP benchmarks [49]. This tests the ability of NACs to handle long streams of low-level and unprocessed data.

The results are presented in Table 3. We find that NACs are competitive, both against a general-purpose baseline (our implementation of Perceiver IO) and other third-party baselines, including full-attention transformers and the Linformer. This confirms that NACs are indeed general purpose, and that general-purpose architectures can be effective for handling such data.

### Sample-Conditional Circuit Design Generation

In the previous sections, we quantitatively analyzed the performance of a NAC model equipped with a connectivity graph that is learned, but fixed at inference time. In this section, we investigate sample-conditional circuit generation without regularizing the graph structure. Concretely, we study the circuit designs generated by a conditional NAC trained to perform a multi-modal reasoning task. In our experiments with conditional NACs, the circuit generator was implemented by a simple cross attention layer followed by a feed forward network (FFN), and a self-attention layer followed by

\begin{table}
\begin{tabular}{l c} \hline \hline Method & Test Accuracy \\ \hline Hierarchical Perceiver (No MAE) [14] & 75.9\% \\ Perceiver IO [14] & 77.4\% \\ Hierarchical Perceiver (with MAE) [14] & 80.6\% \\
**NAC (ours)** (No MAE) & **83.0\%** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Point Cloud Classification.** In this experiment, we compare against results reported in Carreira et al. [14]. In that work, the authors were able to improve over Perceiver IO results through a masked auto-encoding (MAE) pre-training step. We find that, without pre-training, NACs with a Scale-Free sparse prior graph are able to achieve superior test accuracy.

\begin{table}
\begin{tabular}{l c c} \hline \hline \multicolumn{3}{c}{**Test Accuracy**} \\ \hline Method & ListOps [39] & Text Classification [49] \\ \hline Full Attention Transformers [49] & 37.13\% & 65.35\% \\ Linformer [49] & 37.38\% & 56.12\% \\ Perceiver IO (our impl.) & 39.70\% & 66.50\% \\ NAC (ours) & **41.40\%** & **68.18\%** \\ \hline \hline \end{tabular}
\end{table}
Table 3: **ListOps and Text Classification Results.** In this experiment, we train a Perceiver IO and a NAC from scratch on two tasks from the the Long Range Arena [49]: ListOps symbolic processing and text classification from ASCII bytes. We observe that NAC excels in processing long-range dependencies in this setting.

two FFNs run in parallel, see Appendix G for further details. We use Natural Language for Visual Reasoning _for Real_ (NLVR2) dataset [48] which contains natural language sentences grounded in images. The task is to determine whether a natural language statement holds true for a given pair of photos. Solving this task requires reasoning about sets of objects, making comparisons, and determining spatial relations. Accordingly, we would expect that the circuit generator creates circuit designs that are qualitatively different for different logical relations.

**Training.** We condition the circuit generator on natural language captions, and the executor outputs a binary label conditioned both on the image pairs and on the output of the circuit generator. Given that our goal of understanding how conditional NACs perform reasoning, we use a (frozen) pre-trained CLIP [42] backbone to pre-process both texts and images. We select CLIP because it allows us to measure zero-shot performance out-of-the-box (53% validation accuracy, details in Appendix G), and observe that conditional NACs improve on it (obtaining approximately 64% validation accuracy).

**Analysis.** We select a trained model and analyze the behaviour of its circuit generator. In Figure 6, we visualize the 2D embeddings of the link probability matrix \(P\) (see Equation 2), as well as a selection of inferred connectivity graphs for simple (non-compound) natural language statements that capture a subset of the semantic phenomena found in Suhr et al. [48] (see Appendix G). To obtain the former, we flatten the lower-triangular part of the link probability matrix \(P_{ij}\) into a vector. We first reduce this \(\sim\)50k-dimensional vector to 64 dimensions via PCA [31]. Subsequently, TSNE [51] is used to embed the 64 dimensional vector in 2 dimensions. To obtain the plots of module connectivity graphs, we use a force-based graph visualization subroutine in the network analysis library NetworkX [22], wherein the attractive force between two modules is inversely proportional to the distance of their respective signatures. We draw an edge between modules if they are more than 50% likely to be connected with each other, and the module colors are consistent over all shown graphs. **Key observations:****(a)** there is diversity in the generated graph in the conditional setting, even though we do not use a regularizing prior like we did in the image classification task; **(b)** different sentence types are assigned to different graph structures. Further, the generated graphs share a similar structure with two large cliques, with nodes that bridge them.

**In conclusion,** we have shown a framework in Neural Attentive Circuits (NACs) for constructing a general-purpose modular neural architecture that can jointly learn meaningful module parameterizations and connectivity graphs. We have demonstrated empirically that this modular approach yields improvements in few-shot adaptation, computational efficiency, and OOD robustness.

Figure 6: **Link Probability Matrix Embeddings. This figure shows a TSNE [51] embedding of the generated graphs provided different kinds of text input. We color-code text inputs based on the type of reasoning that it requires; here we show _hard cardinality_ which includes a precise number of objects, _soft cardinality_ which includes words like “many” or “few”, _existential_ which only indicates that an object is present, and spatial relationships which use a prepositional phrase to denote a physical relationship. We see by the clustering that the low-dimensional embedding space of the generated graph captures aspects of the reasoning task. We also note that all learned graphs are similarly structured with two large cliques, but the specific arrangement varies widely.**



**Acknowledgements**. This work was partially conducted while Nasim Rahaman was interning at Meta AI. Chris Pal and Martin Weiss thank NSERC for support under the COHESA program and IVADO for support under their AI, Biodiversity and Climate Change Thematic Program. Nasim Rahaman and Bernhard Scholkopf was supported by the German Federal Ministry of Education and Research (BMBF): Tubingen AI Center, FKZ: 01IS18039B, and by the Machine Learning Cluster of Excellence, EXC number 2064/1 - Project number 390727645.

## References

* Achlioptas et al. [2002] D. Achlioptas, F. McSherry, and B. Scholkopf. Sampling techniques for kernel methods. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, _Advances in Neural Information Processing Systems 14_, pages 335-342, Cambridge, MA, USA, 2002. MIT Press.
* Alet et al. [2019] Ferran Alet, Erica Weng, Tomas Lozano-Perez, and Leslie Pack Kaelbling. Neural relational inference with fast modular meta-learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/b294504229c668e750dfcc4ea9617f0a-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/b294504229c668e750dfcc4ea9617f0a-Paper.pdf).
* Andreas et al. [2015] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional question answering with neural module networks. _CoRR_, abs/1511.02799, 2015. URL [http://arxiv.org/abs/1511.02799](http://arxiv.org/abs/1511.02799).
* Bahdanau et al. [2018] Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron C. Courville. Systematic generalization: What is required and can it be learned? _CoRR_, abs/1811.12889, 2018. URL [http://arxiv.org/abs/1811.12889](http://arxiv.org/abs/1811.12889).
* Bahdanau et al. [2019] Dzmitry Bahdanau, Harm de Vries, Timothy J. O'Donnell, Shikhar Murty, Philippe Beaudoin, Yoshua Bengio, and Aaron C. Courville. CLOSURE: assessing systematic generalization of CLEVR models. _CoRR_, abs/1912.05783, 2019. URL [http://arxiv.org/abs/1912.05783](http://arxiv.org/abs/1912.05783).
* Barabasi and Albert [1999] Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. _science_, 286(5439):509-512, 1999.
* Barabasi and Bonabeau [2003] Albert-Laszlo Barabasi and E. Bonabeau. Scale-free networks. _Scientific American_, 288(60-69), 2003. URL [http://www.nd.edu/~networks/PDF/Scale-Free%20Sci%20Amer%20May03.pdf](http://www.nd.edu/~networks/PDF/Scale-Free%20Sci%20Amer%20May03.pdf).
* Bartle [1995] Robert G. Bartle. _The Elements of Integration and Lebesgue Measure_. John Wiley & Sons, New York, 1995.
* Bengio et al. [2015] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models. _arXiv preprint arXiv:1511.06297_, 2015.
* Bengio et al. [2013] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. _arXiv preprint arXiv:1308.3432_, 2013.
* Bengio et al. [2020] Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan Rosemary Ke, Sebastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal mechanisms. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=ryxwIgBFPS](https://openreview.net/forum?id=ryxwIgBFPS).
* Bengio et al. [2021] Yoshua Bengio, Tristan Deleu, Edward J. Hu, Salem Lahlou, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. _CoRR_, abs/2111.09266, 2021. URL [https://arxiv.org/abs/2111.09266](https://arxiv.org/abs/2111.09266).
* Carion et al. [2020] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers, 2020. URL [https://arxiv.org/abs/2005.12872](https://arxiv.org/abs/2005.12872).
* Carreira et al. [2022] Joao Carreira, Skanda Koppula, Daniel Zoran, Adria Recasens, Catalin Ionescu, Olivier Henaff, Evan Shelhamer, Relja Arandjelovic, Matt Botvinick, Oriol Vinyals, Karen Simonyan, Andrew Zisserman, and Andrew Jaegle. Hierarchical perceiver, 2022. URL [https://arxiv.org/abs/2202.10890](https://arxiv.org/abs/2202.10890).

* Condon and Karp [1999] Anne Condon and Richard M. Karp. Algorithms for graph partitioning on the planted partition model. 18:116-140, 1999.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Erdos and Renyi [1959] P. Erdos and A. Renyi. On random graphs i. _Publicationes Mathematicae Debrecen_, 6:290, 1959.
* Erdos et al. [1960] Paul Erdos, Alfred Renyi, et al. On the evolution of random graphs. _Publ. Math. Inst. Hung. Acad. Sci_, 5(1):17-60, 1960.
* Glasscock [2016] Daniel Glasscock. What is a graphon? 2016. doi: 10.48550/ARXIV.1611.00718. URL [https://arxiv.org/abs/1611.00718](https://arxiv.org/abs/1611.00718).
* Goyal et al. [2019] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Scholkopf. Recurrent independent mechanisms. _CoRR_, abs/1909.10893, 2019. URL [http://arxiv.org/abs/1909.10893](http://arxiv.org/abs/1909.10893).
* Gupta et al. [2019] Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. Neural module networks for reasoning over text. _CoRR_, abs/1912.04971, 2019. URL [http://arxiv.org/abs/1912.04971](http://arxiv.org/abs/1912.04971).
* Hagberg et al. [2008] Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008.
* Hawthorne et al. [2022] Curtis Hawthorne, Andrew Jaegle, Catalina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, Hannah Sheahan, Neil Zeghidour, Jean-Baptiste Alayrac, Joao Carreira, and Jesse Engel. General-purpose, long-context autoregressive modeling with perceiver ar, 2022. URL [https://arxiv.org/abs/2202.07765](https://arxiv.org/abs/2202.07765).
* Hendrycks et al. [2020] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. _CoRR_, abs/2006.16241, 2020. URL [https://arxiv.org/abs/2006.16241](https://arxiv.org/abs/2006.16241).
* Holland et al. [1983] Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps. _Social networks_, 5(2):109-137, 1983.
* Hu et al. [2017] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, 2017.
* Hudson and Manning [2018] Drew A Hudson and Christopher D Manning. Compositional attention networks for machine reasoning. In _International Conference on Learning Representations_, 2018.
* Jaegle et al. [2021] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Henaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver io: A general architecture for structured inputs and outputs, 2021. URL [https://arxiv.org/abs/2107.14795](https://arxiv.org/abs/2107.14795).
* Jaegle et al. [2021] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention. _CoRR_, abs/2103.03206, 2021. URL [https://arxiv.org/abs/2103.03206](https://arxiv.org/abs/2103.03206).
* Johnson et al. [2016] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. _CoRR_, abs/1612.06890, 2016. URL [http://arxiv.org/abs/1612.06890](http://arxiv.org/abs/1612.06890).
* Jolliffe [1986] I.T. Jolliffe. _Principal Component Analysis_. Springer Verlag, 1986.

* Lee et al. [2018] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer. _CoRR_, abs/1810.00825, 2018. URL [http://arxiv.org/abs/1810.00825](http://arxiv.org/abs/1810.00825).
* Lee et al. [2021] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision transformer for small-size datasets, 2021. URL [https://arxiv.org/abs/2112.13492](https://arxiv.org/abs/2112.13492).
* Lin et al. [2019] Zehui Lin, Pengfei Liu, Luyao Huang, Junkun Chen, Xipeng Qiu, and Xuanjing Huang. Dropattention: A regularization method for fully-connected self-attention networks. _CoRR_, abs/1907.11065, 2019. URL [http://arxiv.org/abs/1907.11065](http://arxiv.org/abs/1907.11065).
* Liu et al. [2022] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. _arXiv preprint arXiv:2201.03545_, 2022.
* Maddison et al. [2016] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. _CoRR_, abs/1611.00712, 2016. URL [http://arxiv.org/abs/1611.00712](http://arxiv.org/abs/1611.00712).
* Mao et al. [2019] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. _CoRR_, abs/1904.12584, 2019. URL [http://arxiv.org/abs/1904.12584](http://arxiv.org/abs/1904.12584).
* Miyauchi and Kawase [2016] Atsushi Miyauchi and Yasushi Kawase. Ring of cliques network. 1 2016. doi: 10.1371/journal.pone.0147805.g002. URL [https://plos.figshare.com/articles/figure/_Ring_of_cliques_network_/1642440](https://plos.figshare.com/articles/figure/_Ring_of_cliques_network_/1642440).
* Nangia and Bowman [2018] Nikita Nangia and Samuel R. Bowman. Listops: A diagnostic dataset for latent tree learning, 2018. URL [https://arxiv.org/abs/1804.06028](https://arxiv.org/abs/1804.06028).
* Perez et al. [2017] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. _CoRR_, abs/1709.07871, 2017. URL [http://arxiv.org/abs/1709.07871](http://arxiv.org/abs/1709.07871).
* Peters et al. [2017] Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. _Elements of Causal Inference: Foundations and Learning Algorithms_. The MIT Press, 2017. ISBN 0262037319.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. _CoRR_, abs/2103.00020, 2021. URL [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020).
* Raffel et al. [2019] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _CoRR_, abs/1910.10683, 2019. URL [http://arxiv.org/abs/1910.10683](http://arxiv.org/abs/1910.10683).
* Rahaman et al. [2020] Nasim Rahaman, Anirudh Goyal, Muhammad Waleed Gondal, Manuel Wuthrich, Stefan Bauer, Yash Sharma, Yoshua Bengio, and Bernhard Scholkopf. S2rms: Spatially structured recurrent modules. _CoRR_, abs/2007.06533, 2020. URL [https://arxiv.org/abs/2007.06533](https://arxiv.org/abs/2007.06533).
* Rahaman et al. [2021] Nasim Rahaman, Muhammad Waleed Gondal, Shruti Joshi, Peter V. Gehler, Yoshua Bengio, Francesco Locatello, and Bernhard Scholkopf. Dynamic inference with neural interpreters. _CoRR_, abs/2110.06399, 2021. URL [https://arxiv.org/abs/2110.06399](https://arxiv.org/abs/2110.06399).
* Scholkopf [2019] Bernhard Scholkopf. Causality for machine learning. _CoRR_, abs/1911.10500, 2019. URL [http://arxiv.org/abs/1911.10500](http://arxiv.org/abs/1911.10500).
* Scholkopf et al. [2021] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. _Proceedings of the IEEE_, 109(5):612-634, 2021.
* Suhr et al. [2018] Alane Suhr, Stephanie Zhou, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. _CoRR_, abs/1811.00491, 2018. URL [http://arxiv.org/abs/1811.00491](http://arxiv.org/abs/1811.00491).

* Tay et al. [2021] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=qY9Vw-grC2k](https://openreview.net/forum?id=qY9Vw-grC2k).
* Touvron et al. [2021] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _International Conference on Machine Learning_, pages 10347-10357. PMLR, 2021.
* Maaten and Hinton [2008] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. _Journal of Machine Learning Research_, 9:2579-2605, 2008. URL [http://www.jmlr.org/papers/v9/vandermaaten08a.html](http://www.jmlr.org/papers/v9/vandermaaten08a.html).
* Xu et al. [2022] Kan Xu, Hamsa Bastani, and Osbert Bastani. Robust generalization of quadratic neural networks via function identification, 2022. URL [https://openreview.net/forum?id=Xx4MNjSmQQ9](https://openreview.net/forum?id=Xx4MNjSmQQ9).